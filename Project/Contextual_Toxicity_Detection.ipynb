{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRjqfT9wpa+ZYXhYaW+hkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwesh1010/CS_6301-DL_for_NLP-/blob/main/Project/Contextual_Toxicity_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing various packages and dependency**"
      ],
      "metadata": {
        "id": "CZjjEc44qgO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install tensorflow-addons\n",
        "import gc\n",
        "import os\n",
        "import emoji as emoji\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import AutoModel,BertModel, BertTokenizer,AutoTokenizer,TFAutoModelForSequenceClassification,AdamW\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXv4CXpr-7Q_",
        "outputId": "121fb7e5-b229-4aaf-9096-babe2e082f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJwsfqnc__t3",
        "outputId": "e2971ec8-c0d8-49bc-ff73-0a2334b99ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setting up the device for GPU usage\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "JY-CW7qk_31l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebec322-9a3b-4b34-a7cf-25fd0442821d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Dataset**"
      ],
      "metadata": {
        "id": "LUDnYelgrbLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/labeled_data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "z41uqEreGd9g",
        "outputId": "f065c3a3-df91-420d-be4c-a04f7ee64033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
              "0           0      3            0                   0        3      2   \n",
              "1           1      3            0                   3        0      1   \n",
              "2           2      3            0                   3        0      1   \n",
              "3           3      3            0                   2        1      1   \n",
              "4           4      6            0                   6        0      1   \n",
              "\n",
              "                                               tweet  \n",
              "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
              "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
              "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
              "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
              "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3074daf-55ff-46cc-8289-96fe0f75a490\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3074daf-55ff-46cc-8289-96fe0f75a490')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3074daf-55ff-46cc-8289-96fe0f75a490 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3074daf-55ff-46cc-8289-96fe0f75a490');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x ='class', data = df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "5acOst1VGf0N",
        "outputId": "d64df469-1fc5-473e-9aff-ee33ed8410fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='class', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preprocessing**"
      ],
      "metadata": {
        "id": "K5s1gdsvroYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['count', 'hate_speech', 'offensive_language', 'neither','Unnamed: 0'], axis=1)\n",
        "   "
      ],
      "metadata": {
        "id": "pC8GFYi__HzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_tweets(values):\n",
        "    new_values = list()\n",
        "    # Emoticons\n",
        "    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n",
        "                 ';-)',\n",
        "                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n",
        "                 '=)',\n",
        "                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n",
        "                 ':>',\n",
        "                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n",
        "                 ':-|',\n",
        "                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n",
        "    for value in values:\n",
        "    # Remove dots\n",
        "        text = value.replace(\".\", \"\").lower()\n",
        "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "        users = re.findall(\"[@]\\w+\", text)\n",
        "        for user in users:\n",
        "            text = text.replace(user, \"<user>\")\n",
        "        urls = re.findall(r'(https?://[^\\s]+)', text)\n",
        "        if len(urls) != 0:\n",
        "            for url in urls:\n",
        "                text = text.replace(url, \"<url >\")\n",
        "        numbers = re.findall('[0-9]+', text)\n",
        "        for number in numbers:\n",
        "            text = text.replace(number, \"<number >\")\n",
        "        for emo in text:\n",
        "            if emoji.is_emoji(emo):\n",
        "                text = text.replace(emo, \"<emoticon >\")\n",
        "        for emo in emoticons:\n",
        "            text = text.replace(emo, \"<emoticon >\")\n",
        "\n",
        "        text = text.replace('#', \"<hashtag >\")\n",
        "        text = re.sub(r\"([?.!,¿])\", r\" \", text)\n",
        "        text = \"\".join(l for l in text if l not in string.punctuation)\n",
        "        text = re.sub(r'[\" \"]+', \" \", text)\n",
        "        new_values.append(text)\n",
        "    return new_values"
      ],
      "metadata": {
        "id": "dFhBLoc5_sL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = pre_process_tweets(df['tweet'].tolist())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-mM9Laq_LWyj",
        "outputId": "8de4d093-b6ef-4112-b31f-954617f40433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class                                              tweet\n",
              "0      2   rt mayasolovely as a woman you shouldn t comp...\n",
              "1      1   rt mleew boy dats coldtyga dwn bad for cuffin...\n",
              "2      1   rt urkindofbrand dawg rt sbaby life you ever ...\n",
              "3      1   rt c g anderson viva based she look like a tr...\n",
              "4      1   rt shenikaroberts the shit you hear about me ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05e35c4a-6b62-47b6-a1c6-4c7b1a183142\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>rt mayasolovely as a woman you shouldn t comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>rt mleew boy dats coldtyga dwn bad for cuffin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>rt urkindofbrand dawg rt sbaby life you ever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>rt c g anderson viva based she look like a tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>rt shenikaroberts the shit you hear about me ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05e35c4a-6b62-47b6-a1c6-4c7b1a183142')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05e35c4a-6b62-47b6-a1c6-4c7b1a183142 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05e35c4a-6b62-47b6-a1c6-4c7b1a183142');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df['tweet']\n",
        "labels = df['class']\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(data, labels,\n",
        "                             random_state=2018, test_size=0.2, stratify=labels)\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                         random_state=2018, test_size=0.5, stratify=temp_labels)\n",
        "print(\"Train_size:\",len(train_text))\n",
        "print(\"Val_size:\",len(train_text))\n",
        "print(\"Test_size:\",len(train_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yaZ5eJrOEhS",
        "outputId": "5d7a04cf-94ad-4357-f02d-b67c5742d638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_size: 19826\n",
            "Val_size: 19826\n",
            "Test_size: 19826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FineTuning using AutoModel and Pretraining using AutoTokenizer BERT**"
      ],
      "metadata": {
        "id": "StYGk3CcfC_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "4jzD63FQjr0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "NNoIRDhJAgYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_train_data = dict(tokenizer(train_text[:].tolist(), padding='max_length', truncation=True))\n",
        "tokenized_val_data = dict(tokenizer(val_text[:].tolist(), padding='max_length', truncation=True))\n",
        "tokenized_test_data = dict(tokenizer(test_text[:].tolist(), padding='max_length', truncation=True))"
      ],
      "metadata": {
        "id": "C3cCiGmIQhFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data"
      ],
      "metadata": {
        "id": "CLwwLlC0jwH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_for_final_model = tf.data.Dataset.from_tensor_slices((tokenized_train_data,tf.keras.utils.to_categorical(train_labels, num_classes=3)))\n",
        "train_set_for_final_model =train_set_for_final_model.batch(8)\n",
        "val_set_for_final_model = tf.data.Dataset.from_tensor_slices((tokenized_val_data,tf.keras.utils.to_categorical(val_labels, num_classes=3)))\n",
        "val_set_for_final_model =val_set_for_final_model.batch(8)\n",
        "test_set_for_final_model = tf.data.Dataset.from_tensor_slices((tokenized_test_data,tf.keras.utils.to_categorical(test_labels, num_classes=3)))\n",
        "test_set_for_final_model =test_set_for_final_model.batch(8)"
      ],
      "metadata": {
        "id": "OgQ_JTfNTYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "POy8u8yWjy0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    metrics=[tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.Recall(class_id=0, name='Recall_0'),tf.keras.metrics.Recall(class_id=1, name='Recall_1'),tf.keras.metrics.Recall(class_id=2, name='Recall_2'),tf.keras.metrics.Precision(class_id=0, name='Precision_0'),tf.keras.metrics.Precision(class_id=1, name='Precision_1'),tf.keras.metrics.Precision(class_id=2, name='Precision_2')],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNufzG0TB70p",
        "outputId": "0cd22dc4-6f06-41b6-e0a6-a3ddd8fb7095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "LwJisuBIj0QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_set_for_final_model, validation_data=val_set_for_final_model, epochs=3 )"
      ],
      "metadata": {
        "id": "cCJalXbwDK9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a2cd83-2210-4a73-d5b5-30f75248fe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2479/2479 [==============================] - 2408s 962ms/step - loss: 14.3562 - categorical_accuracy: 0.0623 - Recall_0: 0.9878 - Recall_1: 0.0042 - Recall_2: 0.0066 - Precision_0: 0.0581 - Precision_1: 0.7356 - Precision_2: 0.1571 - val_loss: 15.1880 - val_categorical_accuracy: 0.0577 - val_Recall_0: 1.0000 - val_Recall_1: 0.0000e+00 - val_Recall_2: 0.0000e+00 - val_Precision_0: 0.0577 - val_Precision_1: 0.0000e+00 - val_Precision_2: 0.0000e+00\n",
            "Epoch 2/3\n",
            "2479/2479 [==============================] - 2375s 958ms/step - loss: 14.5683 - categorical_accuracy: 0.0577 - Recall_0: 1.0000 - Recall_1: 0.0000e+00 - Recall_2: 0.0000e+00 - Precision_0: 0.0577 - Precision_1: 0.0000e+00 - Precision_2: 0.0000e+00 - val_loss: 15.1880 - val_categorical_accuracy: 0.0577 - val_Recall_0: 1.0000 - val_Recall_1: 0.0000e+00 - val_Recall_2: 0.0000e+00 - val_Precision_0: 0.0577 - val_Precision_1: 0.0000e+00 - val_Precision_2: 0.0000e+00\n",
            "Epoch 3/3\n",
            " 234/2479 [=>............................] - ETA: 34:19 - loss: 14.7749 - categorical_accuracy: 0.0502 - Recall_0: 1.0000 - Recall_1: 0.0000e+00 - Recall_2: 0.0000e+00 - Precision_0: 0.0502 - Precision_1: 0.0000e+00 - Precision_2: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing and Result "
      ],
      "metadata": {
        "id": "uJdqIvhDj2QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_set_for_final_model, verbose=2)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(test_labels, y_pred_bool))"
      ],
      "metadata": {
        "id": "HqScuONSY6yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FineTuning with ANN Model and Pretraining BERT Tokenizer**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Djo595vIpvy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization - BERT Tokenize"
      ],
      "metadata": {
        "id": "6rCVwpJegm3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GA8bXdJ1a136"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenize_data(data, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    for sentence in data:\n",
        "        bert_inp = bert_tokenizer.__call__(sentence, max_length=36,\n",
        "                                           padding='max_length', pad_to_max_length=True,\n",
        "                                           truncation=True, return_token_type_ids=False)\n",
        "\n",
        "        input_ids.append(bert_inp['input_ids'])\n",
        "        attention_masks.append(bert_inp['attention_mask'])\n",
        "    del bert_tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    input_ids = np.asarray(input_ids)\n",
        "    attention_masks = np.array(attention_masks)\n",
        "    labels = np.array(labels)\n",
        "    return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "mDkY_9M0_y8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_masks, labels = Tokenize_data(data, labels)"
      ],
      "metadata": {
        "id": "CB9lC7sN_1-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])"
      ],
      "metadata": {
        "id": "-j_qmcN3pqNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataSet Split (Token Split)"
      ],
      "metadata": {
        "id": "5TnI3AdUgzVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
        "                             random_state=2018, test_size=0.2, stratify=labels)\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                         random_state=2018, test_size=0.5, stratify=temp_labels)\n",
        "\n",
        "del temp_text\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "train_count = len(train_labels)\n",
        "test_count = len(test_labels)\n",
        "val_count = len(val_labels)"
      ],
      "metadata": {
        "id": "xXqHOhsBAQ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing FOR BERT"
      ],
      "metadata": {
        "id": "0kJ82nnNhII_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "NXs4FNnCAZv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd889b7-0114-40f3-c602-e13d813e6438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
        "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
        "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
        "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "9FQAgFjWA2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "h38e5WKACloe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "fx-iS5Diqa8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining ANN Architecture on Top of BERT\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9BvdEqpyhR_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_ANN(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_ANN, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,3)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "0cV0fTrNqb61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model\n",
        "\n"
      ],
      "metadata": {
        "id": "aCTVmOuHhipq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "torch.cuda.empty_cache()\n",
        "model2 = BERT_ANN(bert)\n",
        "# push the model to GPU\n",
        "model2 = model2.to(device)"
      ],
      "metadata": {
        "id": "No5mHXAD08Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model2.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Hb0MUV1CT-",
        "outputId": "78e95af3-3937-4b18-d799-b799e1a3c498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Balancing "
      ],
      "metadata": {
        "id": "MI7GElR7hwLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels                                                    \n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(train_labels), class_weights))\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXqV3id91Zl7",
        "outputId": "2292d7c5-e166-46b2-e73d-1a3bfc336cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: {0: 5.776806526806527, 1: 0.4304759423310752, 2: 1.9845845845845846}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(list(class_weights.values()),dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "8PflZvoE1akX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customized Train Function"
      ],
      "metadata": {
        "id": "fwVDyt1zhzLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "    model2.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    # empty list to save model predictions\n",
        "    total_preds = []\n",
        "    # iterate over batches\n",
        "    total = len(train_dataloader)\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "        step = i+1\n",
        "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # clear previously calculated gradients\n",
        "        model2.zero_grad()\n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        #sent_id = torch.tensor(sent_id).to(device).long()\n",
        "        preds = model2(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        #preds = preds.detach().cpu().numpy()\n",
        "\n",
        "        # append the model predictions\n",
        "        #total_preds.append(preds)\n",
        "        total_preds.append(preds.detach().cpu().numpy())\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n",
        "\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    # returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "Moq8jS311gvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customized Test Function"
      ],
      "metadata": {
        "id": "EYcHXV2Qh9Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    print(\"\\n\\nEvaluating...\")\n",
        "\n",
        "    # deactivate dropout layers\n",
        "    model2.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    total = len(val_dataloader)\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        \n",
        "        step = i+1\n",
        "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # model predictions\n",
        "            preds = model2(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds, labels)\n",
        "\n",
        "            total_loss += float(loss.item())\n",
        "            #preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            #total_preds.append(preds)\n",
        "            total_preds.append(preds.detach().cpu().numpy())\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / (len(val_dataloader)*batch_size)\n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "ViauuZcb1sbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Results "
      ],
      "metadata": {
        "id": "b4_xln93iEe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "epochs = 20\n",
        "for epoch in range(0,epochs):\n",
        "\n",
        "    print(f'\\nEpoch {epoch} / {epochs}:')\n",
        "\n",
        "    # train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    # evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss  \n",
        "\n",
        "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "\n",
        "\n",
        "# get predictions for test data\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model2(test_seq.to(device), test_mask.to(device))\n",
        "    #preds = model(test_seq, test_mask)\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "print(\"Performance:\")\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis=1)\n",
        "print('Classification Report')\n",
        "print(classification_report(test_y, preds))\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy_score(test_y, preds)))"
      ],
      "metadata": {
        "id": "R7fArE1uDnwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Model**"
      ],
      "metadata": {
        "id": "h4oGRcVweWeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class BERT_CNN(nn.Module):\n",
        "\n",
        "\n",
        "#     def __init__(self, bert):\n",
        "#         super(BERT_CNN, self).__init__()\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         self.conv = nn.Conv2d(in_channels=1, out_channels=13, kernel_size=(3, 768), padding=True)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "#         self.fc = nn.Linear(442, 3) # before : 442 with max_length 36 # 806 with max_length 64\n",
        "#         self.flat = nn.Flatten()\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, sent_id, mask):\n",
        "        \n",
        "#         _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True, return_dict=False)\n",
        "#         x = torch.stack(all_layers, dim=1) # stack hidden states\n",
        "#         x = x.unsqueeze(1) # add channel dimension\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.pool(self.dropout(self.relu(self.conv(x))))\n",
        "#         x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
        "#         return self.softmax(x)"
      ],
      "metadata": {
        "id": "c3YTbI1Fd-9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}